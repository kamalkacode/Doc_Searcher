{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd    \n",
    "import io\n",
    "import wikipedia as w\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Stadiums"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### English football stadiums chosen because they have well documented wiki pages ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_football_stadiums_in_England\"\n",
    "#url of page containing list of football stadiums\n",
    "r = requests.get(url)\n",
    "soup = BS(r.text)    #BS objecct for the url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Wembley Stadium', u'Tottenham Hotspur', u'Old Trafford', u'Emirates Stadium', u'London Stadium', u'City of Manchester Stadium', u'Anfield', u\"St James' Park\", u'Stadium of Light', u'Villa Park']\n"
     ]
    }
   ],
   "source": [
    "stadium_list = soup.find_all('table',{'class' : 'wikitable'})\n",
    "stadium_list = stadium_list[0] #2 tables only first table\n",
    "stadium_list = stadium_list.find('tbody') #body of table selected\n",
    "stadium_list = stadium_list.find_all('tr') #select all rows\n",
    "\n",
    "stadiums = []\n",
    "\n",
    "for i in stadium_list:\n",
    "    tmp = i.find('a')\n",
    "    if tmp != None:\n",
    "        stadiums.append(tmp.text)\n",
    "\n",
    "print stadiums[:10]   #print first 10 stadiums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wembley Stadium\n",
      "6\n",
      "6\n",
      "Tottenham Hotspur F.C.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-35bb9510b39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#             print len(doc_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m180\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/kush/Desktop/Wiki_Data_2/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Stadium_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/codecs.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "ix = 0\n",
    "# The following loop uses wikipedia api (which is shit), to get \n",
    "# wikipedia pg object (we pass title as argument) .Sometimes one title \n",
    "# can have multiple disambiguation , this will be caught as exception\n",
    "# and we can print all possible alternate titles. We can then pass the\n",
    "# appropriate string .\n",
    "\n",
    "# pg.content prints entire string , print it and you will see \n",
    "# '== %title% ==' for level 2 headers and '=== %title% ===' for level \n",
    "# 3 headers . I have divided it such that text docs are seperated by \n",
    "# level 2 headers , and any text doc having less than 200 words is discarded\n",
    "\n",
    "cnt = 0\n",
    "ix = 0\n",
    "while ix < len(stadiums):\n",
    "    try :\n",
    "        pg = w.WikipediaPage(stadiums[ix])  # pg object contains info about wiki page of stad[ix]\n",
    "        url =  pg.url                       # gives url of wiki page\n",
    "\n",
    "        r = requests.get(url)               \n",
    "        soup = BS(r.text)                   # soup object of wikipage\n",
    "        \n",
    "        title = soup.find('h1').text        #h1 is the title of page(this is first heading , getting it through BS4 as not included in pg.content)\n",
    "        print title                         #prints stadium name \n",
    "\n",
    "        string = pg.content                       # whole page as unicode string\n",
    "        pattern = r'\\n==\\s.+\\s==\\n'               # Regex pattern for headings \n",
    "        headings  = re.findall(pattern, string)   # Each unnique heading \n",
    "        \n",
    "        \n",
    "        # ind is a list which contains tuples as entries \n",
    "        # tuple has info (heading name,index of start )\n",
    "        # heading name modified with _ and exchanged '/' for '\\'\n",
    "        # as these can give problems \n",
    "        ind = [('_'+title.strip('\\n').strip('=').replace('/','\\\\').replace(' ','_')+'_title',0)]\n",
    "        # ^^ we explicitly give first heading and index 0 ,as not part of pg.content\n",
    "        \n",
    "        # we use below loop to get doc_text which contains all words\n",
    "        # inside headings\n",
    "        for i in headings :\n",
    "            pos = string.find(i)\n",
    "            i = i.strip('\\n').strip('=').replace('/','\\\\').replace(' ','_')\n",
    "            ind.append((i,pos))\n",
    "        ind.append((None,-1))\n",
    "        \n",
    "        page_docs = 0   # num of docs > 180 in length per title\n",
    "                        # just so we get idea how many page_docs\n",
    "                        # qualified at each iteration\n",
    "        for i in range(len(ind)-1) :\n",
    "            \n",
    "            doc_text =  string[ind[i][1]:ind[i+1][1]-1]\n",
    "            # doc_text is content under heading\n",
    "            # if doc_text has more than 180 words ,\n",
    "            # make a document and store in folder\n",
    "            # store file as \n",
    "            # your_Directory/Stadium_%stadium_name%_%topic_name%\n",
    "            \n",
    "#             print 'name of document '+ ind[i][0]\n",
    "#             print len(doc_text.split())\n",
    "            if len(doc_text.split()) >= 180 :\n",
    "                f = io.open(\"/home/kush/Desktop/Wiki_Data_2/\" + 'Stadium_' + title.replace(' ','_') + ind[i][0], \"w+\", encoding = 'utf-8')\n",
    "                f.write(doc_text)\n",
    "                f.close()\n",
    "                cnt += 1\n",
    "                page_docs +=1\n",
    "        print page_docs\n",
    "        print cnt\n",
    "        ix = ix+1\n",
    "                \n",
    "    except Exception as e:         # IMP\n",
    "        print '!'*100              # also try by uncommenting below\n",
    "        print 'EXCEPTION!!!'       # section     \n",
    "        ix += 1\n",
    "#        print e     \n",
    "        continue\n",
    "#         print 'Enter another string'  # Enter one of the strings from the disambiguation   \n",
    "#         inp = raw_input()\n",
    "#         if inp == 'n':\n",
    "#             ix = ix + 1\n",
    "#             continue\n",
    "#         else : \n",
    "#             stadiums[ix] = inp\n",
    "#             continue\n",
    "print 'number of docs = '+ str(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_dog_breeds\"\n",
    "r = requests.get(url)\n",
    "soup = BS(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_list = soup.find_all('table',{'class' : 'wikitable'})\n",
    "dog_list = dog_list[0].find_all('tr') # Fetch all rows\n",
    "nrows = len(dog_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Affenpinscher', u'Afghan Hound', u'Afghan Shepherd', u'Aidi', u'Airedale Terrier', u'Akbash', u'Akita', u'Alano Espa\\xf1ol', u'Alaskan husky', u'Alaskan Klee Kai']\n"
     ]
    }
   ],
   "source": [
    "dogs = []\n",
    "for i in dog_list[1: nrows -1]:\n",
    "    tmp = (i.find('td').find('a')).text\n",
    "    dogs.append(tmp)\n",
    "\n",
    "print dogs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Page id \"Slovakian Wirehaired Pointer\" does not match any pages. Try another id!\n",
      "Enter another string\n",
      "Slovak Rough-haired Pointer\n",
      "Slovak Rough-haired Pointer\n",
      "1\n",
      "918\n",
      "Smaland Hound\n",
      "2\n",
      "920\n",
      "Small Greek Domestic Dog\n",
      "1\n",
      "921\n",
      "Soft-coated Wheaten Terrier\n",
      "4\n",
      "925\n",
      "South Russian Ovcharka\n",
      "0\n",
      "925\n",
      "Southern Hound\n",
      "1\n",
      "926\n",
      "Spanish Mastiff\n",
      "1\n",
      "927\n",
      "Spanish Water Dog\n",
      "2\n",
      "929\n",
      "Spinone Italiano\n",
      "3\n",
      "932\n",
      "Sporting Lucas Terrier\n",
      "0\n",
      "932\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "\"Saint Bernard\" may refer to: \n",
      "St. Bernard (dog)\n",
      "Bernard of Vienne\n",
      "Bernard of Menthon\n",
      "Bernard of Thiron\n",
      "Bernard degli Uberti\n",
      "Bernard of Clairvaux\n",
      "Bernardo Tolomei\n",
      "Bernard of Corleone\n",
      "Saint-Bernard-de-Lacolle, Quebec\n",
      "Saint-Bernard, Quebec\n",
      "Saint-Bernard-de-Michaudville, Quebec\n",
      "L'Isle-aux-Coudres, Quebec\n",
      "St. Bernard, Nova Scotia\n",
      "Saint-Bernard, Ain\n",
      "Saint-Bernard, Côte-d'Or\n",
      "Saint-Bernard, Haut-Rhin\n",
      "Saint-Bernard, Isère\n",
      "Saint Bernard, Southern Leyte\n",
      "Great St Bernard Pass\n",
      "Great St Bernard Tunnel\n",
      "San Bernardino Pass\n",
      "St. Bernard, Ohio\n",
      "St. Bernard Parish, Louisiana\n",
      "Saint Bernard, Louisiana\n",
      "Saint Bernard, Nebraska\n",
      "St. Bernard Township, Platte County, Nebraska\n",
      "San Bernard River\n",
      "San Bernard National Wildlife Refuge\n",
      "Dunnes Stores\n",
      "Texan schooner San Bernard\n",
      "San Bernardino (disambiguation)\n",
      "San Bernardo (disambiguation)\n",
      "São Bernardo (disambiguation)\n",
      "St Bernard Pass (disambiguation)\n",
      "St. Bernard's School (disambiguation)\n",
      "Enter another string\n",
      "St. Bernard (dog)\n",
      "St. Bernard (dog)\n",
      "3\n",
      "935\n",
      "Stabyhoun\n",
      "3\n",
      "938\n",
      "Staffordshire Bull Terrier\n",
      "2\n",
      "940\n",
      "Stephens Cur\n",
      "0\n",
      "940\n",
      "Styrian Coarse-haired Hound\n",
      "0\n",
      "940\n",
      "Sussex Spaniel\n",
      "2\n",
      "942\n",
      "Swedish Lapphund\n",
      "1\n",
      "943\n",
      "Swedish Vallhund\n",
      "0\n",
      "943\n",
      "Tahltan Bear Dog\n",
      "2\n",
      "945\n",
      "Taigan\n",
      "1\n",
      "946\n",
      "Taiwan Dog\n",
      "5\n",
      "951\n",
      "Talbot\n",
      "3\n",
      "954\n",
      "Tamaskan Dog\n",
      "1\n",
      "955\n",
      "Teddy Roosevelt Terrier\n",
      "3\n",
      "958\n",
      "Telomian\n",
      "0\n",
      "958\n",
      "Tenterfield Terrier\n",
      "1\n",
      "959\n",
      "Terceira Mastiff\n",
      "1\n",
      "960\n",
      "Thai Bangkaew Dog\n",
      "2\n",
      "962\n",
      "Thai Ridgeback\n",
      "3\n",
      "965\n",
      "Tibetan Mastiff\n",
      "4\n",
      "969\n",
      "Tibetan Spaniel\n",
      "3\n",
      "972\n",
      "Tibetan Terrier\n",
      "4\n",
      "976\n",
      "Tornjak\n",
      "3\n",
      "979\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "\"Tosa\" may refer to: \n",
      "Kōchi Prefecture\n",
      "Kōchi Prefecture\n",
      "Kōchi Prefecture\n",
      "Tosa, Kōchi\n",
      "Tosa, Kōchi (town)\n",
      "Tosa District, Kōchi\n",
      "Tosa province\n",
      "Tosa dialect\n",
      "Tosa school\n",
      "Tosa Mitsunobu\n",
      "Tosa Mitsuoki\n",
      "tataki\n",
      "Reiko Tosa\n",
      "Reiko Tosa\n",
      "Tosa (dog)\n",
      "Tosa-class battleship\n",
      "Japanese battleship Tosa\n",
      "Sharp Zaurus\n",
      "Cima Tosa\n",
      "Cima Tosa\n",
      "Tosa d'Alp\n",
      "Tosa corner or hairpin\n",
      "Wauwatosa, Wisconsin\n",
      "Enter another string\n",
      "Tosa (dog)\n",
      "Tosa (dog)\n",
      "0\n",
      "979\n",
      "Toy Fox Terrier\n",
      "1\n",
      "980\n",
      "Toy Manchester Terrier\n",
      "1\n",
      "981\n",
      "Toy Trawler Spaniel\n",
      "0\n",
      "981\n",
      "Transylvanian Hound\n",
      "2\n",
      "983\n",
      "Treeing Cur\n",
      "1\n",
      "984\n",
      "Treeing Tennessee Brindle\n",
      "0\n",
      "984\n",
      "Treeing Walker Coonhound\n",
      "1\n",
      "985\n",
      "Trigg Hound\n",
      "1\n",
      "986\n",
      "Tweed Water Spaniel\n",
      "2\n",
      "988\n",
      "Tyrolean Hound\n",
      "0\n",
      "988\n",
      "Cimarrón Uruguayo\n",
      "1\n",
      "989\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Page id \"Valencian Ratter\" does not match any pages. Try another id!\n",
      "Enter another string\n",
      "Gos Rater Valencià\n",
      "Gos Rater Valencià\n",
      "0\n",
      "989\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Page id \"Villano de Las Encartaciones\" does not match any pages. Try another id!\n",
      "Enter another string\n",
      "Villano de las Encartaciones\n",
      "Villano de las Encartaciones\n",
      "0\n",
      "989\n",
      "Vizsla\n",
      "2\n",
      "991\n",
      "Volpino Italiano\n",
      "1\n",
      "992\n",
      "Weimaraner\n",
      "3\n",
      "995\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Page id \"Welsh Corgi, Cardigan\" does not match any pages. Try another id!\n",
      "Enter another string\n",
      "Cardigan Welsh Corgi\n",
      "Cardigan Welsh Corgi\n",
      "2\n",
      "997\n",
      "Welsh Sheepdog\n",
      "2\n",
      "999\n",
      "Welsh Springer Spaniel\n",
      "4\n",
      "1003\n",
      "Welsh Terrier\n",
      "1\n",
      "1004\n",
      "West Highland White Terrier\n",
      "6\n",
      "1010\n",
      "West Siberian Laika\n",
      "4\n",
      "1014\n",
      "Westphalian Dachsbracke\n",
      "2\n",
      "1016\n",
      "Wetterhoun\n",
      "1\n",
      "1017\n",
      "Whippet\n",
      "5\n",
      "1022\n",
      "White Shepherd\n",
      "5\n",
      "1027\n",
      "Wirehaired Pointing Griffon\n",
      "2\n",
      "1029\n",
      "Wirehaired Vizsla\n",
      "2\n",
      "1031\n",
      "Xiasi Dog\n",
      "0\n",
      "1031\n",
      "Yorkshire Terrier\n",
      "5\n",
      "1036\n",
      "number of docs = 1036\n"
     ]
    }
   ],
   "source": [
    "cnt = 917\n",
    "ix = 444\n",
    "while ix < len(dogs):\n",
    "    try :\n",
    "        pg = w.WikipediaPage(dogs[ix])  # pg object contains info about wiki page of dog[ix]\n",
    "        url =  pg.url                       # gives url of wiki page\n",
    "\n",
    "        r = requests.get(url)               \n",
    "        soup = BS(r.text)                   # soup object of wikipage\n",
    "        \n",
    "        title = soup.find('h1').text        #h1 is the title of page(this is first heading , getting it through BS4 as not included in pg.content)\n",
    "        print title                         #prints dog name \n",
    "\n",
    "        string = pg.content                       # whole page as unicode string\n",
    "        pattern = r'\\n==\\s.+\\s==\\n'               # Regex pattern for headings \n",
    "        headings  = re.findall(pattern, string)   # Each unnique heading \n",
    "        \n",
    "        \n",
    "        # ind is a list which contains tuples as entries \n",
    "        # tuple has info (heading name,index of start )\n",
    "        # heading name modified with _ and exchanged '/' for '\\'\n",
    "        # as these can give problems \n",
    "        ind = [('_'+title.strip('\\n').strip('=').replace('/','\\\\').replace(' ','_')+'_title',0)]\n",
    "        # ^^ we explicitly give first heading and index 0 ,as not part of pg.content\n",
    "        \n",
    "        # we use below loop to get doc_text which contains all words\n",
    "        # inside headings\n",
    "        for i in headings :\n",
    "            pos = string.find(i)\n",
    "            i = i.strip('\\n').strip('=').replace('/','\\\\').replace(' ','_')\n",
    "            ind.append((i,pos))\n",
    "        ind.append((None,-1))\n",
    "        \n",
    "        page_docs = 0   # num of docs > 180 in length per title\n",
    "                        # just so we get idea how many page_docs\n",
    "                        # qualified at each iteration\n",
    "        for i in range(len(ind)-1) :\n",
    "            \n",
    "            doc_text =  string[ind[i][1]:ind[i+1][1]-1]\n",
    "            # doc_text is content under heading\n",
    "            # if doc_text has more than 180 words ,\n",
    "            # make a document and store in folder\n",
    "            # store file as \n",
    "            # your_Directory/Dog_%breed_name%_%topic_name%\n",
    "            \n",
    "#             print 'name of document '+ ind[i][0]\n",
    "#             print len(doc_text.split())\n",
    "            if len(doc_text.split()) >= 180 :\n",
    "                f = io.open(\"/home/kush/Desktop/Wiki_Data_2/Dogs/\" + 'Dog_' + title.replace(' ','_') + ind[i][0], \"w+\", encoding = 'utf-8')\n",
    "                f.write(doc_text)\n",
    "                f.close()\n",
    "                cnt += 1\n",
    "                page_docs +=1\n",
    "        print page_docs\n",
    "        print cnt\n",
    "        ix = ix+1\n",
    "                \n",
    "    except Exception as e:         # IMP\n",
    "        print '!'*100              # also try by uncommenting below\n",
    "        print 'EXCEPTION!!!'       # section     \n",
    "        ix += 1\n",
    "        print e     \n",
    "#         continue\n",
    "        print 'Enter another string'  # Enter one of the strings from the disambiguation   \n",
    "        inp = raw_input()             # or 'n' to skip\n",
    "        if inp == 'n':\n",
    "            ix = ix + 1\n",
    "            continue\n",
    "        else : \n",
    "            dogs[ix] = inp\n",
    "            continue\n",
    "print 'number of docs = '+ str(cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs.index('Slovakian Wirehaired Pointer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographies of Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
    "r = requests.get(url)\n",
    "soup = BS(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "country_list = soup.find_all('table',{'class' : 'wikitable'})\n",
    "country_list = country_list[0].find_all('tr') # Fetch all rows\n",
    "print len(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "countries= []\n",
    "for row in country_list :\n",
    "    name = row.find('a')\n",
    "    if name != None :\n",
    "        countries.append(name.text)\n",
    "        \n",
    "print len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "11\n",
      "11\n",
      "India\n",
      "10\n",
      "21\n",
      "United States\n",
      "13\n",
      "34\n",
      "Indonesia\n",
      "8\n",
      "42\n",
      "Brazil\n",
      "9\n",
      "51\n",
      "Pakistan\n",
      "10\n",
      "61\n",
      "Nigeria\n",
      "9\n",
      "70\n",
      "Bangladesh\n",
      "8\n",
      "78\n",
      "Russia\n",
      "8\n",
      "86\n",
      "Japan\n",
      "12\n",
      "98\n",
      "Mexico\n",
      "9\n",
      "107\n",
      "Ethiopia\n",
      "13\n",
      "120\n",
      "Philippines\n",
      "11\n",
      "131\n",
      "Egypt\n",
      "1\n",
      "132\n",
      "Vietnam\n",
      "11\n",
      "143\n",
      "Democratic Republic of the Congo\n",
      "8\n",
      "151\n",
      "Germany\n",
      "7\n",
      "158\n",
      "Iran\n",
      "9\n",
      "167\n",
      "Turkey\n",
      "8\n",
      "175\n",
      "Thailand\n",
      "14\n",
      "189\n",
      "France\n",
      "7\n",
      "196\n",
      "United Kingdom\n",
      "9\n",
      "205\n",
      "Italy\n",
      "8\n",
      "213\n",
      "South Africa\n",
      "9\n",
      "222\n",
      "Tanzania\n",
      "8\n",
      "230\n",
      "Myanmar\n",
      "9\n",
      "239\n",
      "South Korea\n",
      "11\n",
      "250\n",
      "Kenya\n",
      "8\n",
      "258\n",
      "Colombia\n",
      "9\n",
      "267\n",
      "Spain\n",
      "8\n",
      "275\n",
      "Argentina\n",
      "8\n",
      "283\n",
      "Algeria\n",
      "8\n",
      "291\n",
      "Ukraine\n",
      "7\n",
      "298\n",
      "Sudan\n",
      "9\n",
      "307\n",
      "Iraq\n",
      "8\n",
      "315\n",
      "Uganda\n",
      "9\n",
      "324\n",
      "Poland\n",
      "7\n",
      "331\n",
      "Canada\n",
      "8\n",
      "339\n",
      "Morocco\n",
      "10\n",
      "349\n",
      "Saudi Arabia\n",
      "10\n",
      "359\n",
      "Uzbekistan\n",
      "13\n",
      "372\n",
      "Malaysia\n",
      "10\n",
      "382\n",
      "Venezuela\n",
      "10\n",
      "392\n",
      "Afghanistan\n",
      "10\n",
      "402\n",
      "Peru\n",
      "9\n",
      "411\n",
      "Ghana\n",
      "12\n",
      "423\n",
      "Angola\n",
      "9\n",
      "432\n",
      "Nepal\n",
      "10\n",
      "442\n",
      "Yemen\n",
      "7\n",
      "449\n",
      "Mozambique\n",
      "8\n",
      "457\n",
      "Madagascar\n",
      "10\n",
      "467\n",
      "North Korea\n",
      "9\n",
      "476\n",
      "Australia\n",
      "8\n",
      "484\n",
      "Ivory Coast\n",
      "8\n",
      "492\n",
      "Cameroon\n",
      "8\n",
      "500\n",
      "Taiwan\n",
      "13\n",
      "513\n",
      "Niger\n",
      "7\n",
      "520\n",
      "Sri Lanka\n",
      "10\n",
      "530\n",
      "Burkina Faso\n",
      "9\n",
      "539\n",
      "Romania\n",
      "8\n",
      "547\n",
      "Mali\n",
      "8\n",
      "555\n",
      "Syria\n",
      "8\n",
      "563\n",
      "Kazakhstan\n",
      "9\n",
      "572\n",
      "Malawi\n",
      "8\n",
      "580\n",
      "Chile\n",
      "10\n",
      "590\n",
      "Guatemala\n",
      "8\n",
      "598\n",
      "Netherlands\n",
      "9\n",
      "607\n",
      "Ecuador\n",
      "14\n",
      "621\n",
      "Zambia\n",
      "8\n",
      "629\n",
      "Cambodia\n",
      "7\n",
      "636\n",
      "Senegal\n",
      "7\n",
      "643\n",
      "Chad\n",
      "8\n",
      "651\n",
      "Somalia\n",
      "7\n",
      "658\n",
      "Zimbabwe\n",
      "11\n",
      "669\n",
      "South Sudan\n",
      "8\n",
      "677\n",
      "Rwanda\n",
      "10\n",
      "687\n",
      "Guinea\n",
      "9\n",
      "696\n",
      "Tunisia\n",
      "7\n",
      "703\n",
      "Belgium\n",
      "7\n",
      "710\n",
      "Benin\n",
      "10\n",
      "720\n",
      "Bolivia\n",
      "8\n",
      "728\n",
      "Cuba\n",
      "9\n",
      "737\n",
      "Haiti\n",
      "12\n",
      "749\n",
      "Greece\n",
      "7\n",
      "756\n",
      "Burundi\n",
      "7\n",
      "763\n",
      "Czech Republic\n",
      "8\n",
      "771\n",
      "Portugal\n",
      "8\n",
      "779\n",
      "Dominican Republic\n",
      "9\n",
      "788\n",
      "Jordan\n",
      "8\n",
      "796\n",
      "Sweden\n",
      "9\n",
      "805\n",
      "Azerbaijan\n",
      "9\n",
      "814\n",
      "Hungary\n",
      "7\n",
      "821\n",
      "United Arab Emirates\n",
      "13\n",
      "834\n",
      "Belarus\n",
      "9\n",
      "843\n",
      "Honduras\n",
      "7\n",
      "850\n",
      "Tajikistan\n",
      "9\n",
      "859\n",
      "Israel\n",
      "8\n",
      "867\n",
      "Austria\n",
      "7\n",
      "874\n",
      "Papua New Guinea\n",
      "9\n",
      "883\n",
      "Switzerland\n",
      "8\n",
      "891\n",
      "Sierra Leone\n",
      "10\n",
      "901\n",
      "Hong Kong\n",
      "10\n",
      "911\n",
      "Togo\n",
      "7\n",
      "918\n",
      "Paraguay\n",
      "7\n",
      "925\n",
      "Bulgaria\n",
      "8\n",
      "933\n",
      "Serbia\n",
      "9\n",
      "942\n",
      "Laos\n",
      "8\n",
      "950\n",
      "El Salvador\n",
      "9\n",
      "959\n",
      "Libya\n",
      "9\n",
      "968\n",
      "Nicaragua\n",
      "8\n",
      "976\n",
      "Kyrgyzstan\n",
      "8\n",
      "984\n",
      "Lebanon\n",
      "10\n",
      "994\n",
      "Turkmenistan\n",
      "8\n",
      "1002\n",
      "Denmark\n",
      "9\n",
      "1011\n",
      "Singapore\n",
      "11\n",
      "1022\n",
      "Finland\n",
      "8\n",
      "1030\n",
      "Slovakia\n",
      "9\n",
      "1039\n",
      "Republic of the Congo\n",
      "7\n",
      "1046\n",
      "Norway\n",
      "8\n",
      "1054\n",
      "Eritrea\n",
      "15\n",
      "1069\n",
      "Oman\n",
      "7\n",
      "1076\n",
      "Costa Rica\n",
      "9\n",
      "1085\n",
      "New Zealand\n",
      "8\n",
      "1093\n",
      "Ireland\n",
      "8\n",
      "1101\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Central African Republic\n",
      "9\n",
      "1110\n",
      "Liberia\n",
      "9\n",
      "1119\n",
      "Kuwait\n",
      "8\n",
      "1127\n",
      "Croatia\n",
      "8\n",
      "1135\n",
      "Panama\n",
      "9\n",
      "1144\n",
      "Mauritania\n",
      "6\n",
      "1150\n",
      "Bosnia and Herzegovina\n",
      "10\n",
      "1160\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Moldova\n",
      "7\n",
      "1167\n",
      "Uruguay\n",
      "8\n",
      "1175\n",
      "Puerto Rico\n",
      "11\n",
      "1186\n",
      "Mongolia\n",
      "8\n",
      "1194\n",
      "Armenia\n",
      "9\n",
      "1203\n",
      "Albania\n",
      "10\n",
      "1213\n",
      "Lithuania\n",
      "9\n",
      "1222\n",
      "Jamaica\n",
      "10\n",
      "1232\n",
      "Qatar\n",
      "8\n",
      "1240\n",
      "Namibia\n",
      "9\n",
      "1249\n",
      "Botswana\n",
      "10\n",
      "1259\n",
      "Lesotho\n",
      "9\n",
      "1268\n",
      "The Gambia\n",
      "7\n",
      "1275\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Gabon\n",
      "7\n",
      "1282\n",
      "Slovenia\n",
      "10\n",
      "1292\n",
      "Latvia\n",
      "7\n",
      "1299\n",
      "Kosovo\n",
      "9\n",
      "1308\n",
      "Guinea-Bissau\n",
      "7\n",
      "1315\n",
      "Bahrain\n",
      "10\n",
      "1325\n",
      "Trinidad and Tobago\n",
      "10\n",
      "1335\n",
      "Estonia\n",
      "7\n",
      "1342\n",
      "Mauritius\n",
      "11\n",
      "1353\n",
      "East Timor\n",
      "8\n",
      "1361\n",
      "Equatorial Guinea\n",
      "7\n",
      "1368\n",
      "Swaziland\n",
      "7\n",
      "1375\n",
      "Djibouti\n",
      "7\n",
      "1382\n",
      "Fiji\n",
      "8\n",
      "1390\n",
      "Cyprus\n",
      "8\n",
      "1398\n",
      "Comoros\n",
      "8\n",
      "1406\n",
      "Guyana\n",
      "10\n",
      "1416\n",
      "Bhutan\n",
      "10\n",
      "1426\n",
      "Solomon Islands\n",
      "7\n",
      "1433\n",
      "Macau\n",
      "8\n",
      "1441\n",
      "Montenegro\n",
      "9\n",
      "1450\n",
      "Luxembourg\n",
      "7\n",
      "1457\n",
      "Suriname\n",
      "13\n",
      "1470\n",
      "Western Sahara\n",
      "7\n",
      "1477\n",
      "Cape Verde\n",
      "8\n",
      "1485\n",
      "Malta\n",
      "8\n",
      "1493\n",
      "Transnistria\n",
      "9\n",
      "1502\n",
      "Brunei\n",
      "10\n",
      "1512\n",
      "Belize\n",
      "7\n",
      "1519\n",
      "The Bahamas\n",
      "10\n",
      "1529\n",
      "Maldives\n",
      "11\n",
      "1540\n",
      "Iceland\n",
      "7\n",
      "1547\n",
      "Northern Cyprus\n",
      "7\n",
      "1554\n",
      "Vanuatu\n",
      "7\n",
      "1561\n",
      "Barbados\n",
      "13\n",
      "1574\n",
      "New Caledonia\n",
      "9\n",
      "1583\n",
      "French Polynesia\n",
      "6\n",
      "1589\n",
      "Abkhazia\n",
      "7\n",
      "1596\n",
      "Samoa\n",
      "7\n",
      "1603\n",
      "São Tomé and Príncipe\n",
      "7\n",
      "1610\n",
      "Guam\n",
      "11\n",
      "1621\n",
      "Saint Lucia\n",
      "8\n",
      "1629\n",
      "Curaçao\n",
      "10\n",
      "1639\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Kiribati\n",
      "8\n",
      "1647\n",
      "Aruba\n",
      "8\n",
      "1655\n",
      "Saint Vincent and the Grenadines\n",
      "7\n",
      "1662\n",
      "Federated States of Micronesia\n",
      "7\n",
      "1669\n",
      "United States Virgin Islands\n",
      "7\n",
      "1676\n",
      "Jersey\n",
      "10\n",
      "1686\n",
      "Grenada\n",
      "8\n",
      "1694\n",
      "Tonga\n",
      "8\n",
      "1702\n",
      "Seychelles\n",
      "8\n",
      "1710\n",
      "Antigua and Barbuda\n",
      "9\n",
      "1719\n",
      "Isle of Man\n",
      "7\n",
      "1726\n",
      "Andorra\n",
      "11\n",
      "1737\n",
      "Dominica\n",
      "8\n",
      "1745\n",
      "Bermuda\n",
      "8\n",
      "1753\n",
      "Guernsey\n",
      "6\n",
      "1759\n",
      "Cayman Islands\n",
      "7\n",
      "1766\n",
      "American Samoa\n",
      "8\n",
      "1774\n",
      "Northern Mariana Islands\n",
      "7\n",
      "1781\n",
      "Greenland\n",
      "9\n",
      "1790\n",
      "Marshall Islands\n",
      "6\n",
      "1796\n",
      "South Ossetia\n",
      "7\n",
      "1803\n",
      "Faroe Islands\n",
      "8\n",
      "1811\n",
      "Saint Kitts and Nevis\n",
      "8\n",
      "1819\n",
      "Sint Maarten\n",
      "6\n",
      "1825\n",
      "Monaco\n",
      "9\n",
      "1834\n",
      "Liechtenstein\n",
      "10\n",
      "1844\n",
      "Turks and Caicos Islands\n",
      "9\n",
      "1853\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Gibraltar\n",
      "12\n",
      "1865\n",
      "San Marino\n",
      "7\n",
      "1872\n",
      "British Virgin Islands\n",
      "9\n",
      "1881\n",
      "Cook Islands\n",
      "5\n",
      "1886\n",
      "Palau\n",
      "7\n",
      "1893\n",
      "Anguilla\n",
      "6\n",
      "1899\n",
      "Wallis and Futuna\n",
      "5\n",
      "1904\n",
      "Tuvalu\n",
      "11\n",
      "1915\n",
      "Nauru\n",
      "8\n",
      "1923\n",
      "Saint Barthélemy\n",
      "8\n",
      "1931\n",
      "Saint Pierre and Miquelon\n",
      "8\n",
      "1939\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "EXCEPTION!!!\n",
      "Montserrat\n",
      "7\n",
      "1946\n",
      "Falkland Islands\n",
      "8\n",
      "1954\n",
      "Christmas Island\n",
      "9\n",
      "1963\n",
      "Norfolk Island\n",
      "8\n",
      "1971\n",
      "Niue\n",
      "7\n",
      "1978\n",
      "Tokelau\n",
      "9\n",
      "1987\n",
      "Vatican City\n",
      "8\n",
      "1995\n",
      "Cocos (Keeling) Islands\n",
      "5\n",
      "2000\n",
      "Pitcairn Islands\n",
      "7\n",
      "2007\n",
      "number of docs = 2007\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "ix = 0\n",
    "while ix < len(countries):\n",
    "    try :\n",
    "        pg = w.WikipediaPage(countries[ix])  # pg object contains info about wiki page of dog[ix]\n",
    "        url =  pg.url                       # gives url of wiki page\n",
    "\n",
    "        r = requests.get(url)               \n",
    "        soup = BS(r.text)                   # soup object of wikipage\n",
    "        \n",
    "        title = soup.find('h1').text        #h1 is the title of page(this is first heading , getting it through BS4 as not included in pg.content)\n",
    "        print title                         #prints dog name \n",
    "\n",
    "        string = pg.content                       # whole page as unicode string\n",
    "        pattern = r'\\n==\\s.+\\s==\\n'               # Regex pattern for headings \n",
    "        headings  = re.findall(pattern, string)   # Each unnique heading \n",
    "        \n",
    "        \n",
    "        # ind is a list which contains tuples as entries \n",
    "        # tuple has info (heading name,index of start )\n",
    "        # heading name modified with _ and exchanged '/' for '\\'\n",
    "        # as these can give problems \n",
    "        ind = [('_'+title.strip('\\n').strip('=').replace('/','\\\\').replace(' ','_')+'_title',0)]\n",
    "        # ^^ we explicitly give first heading and index 0 ,as not part of pg.content\n",
    "        \n",
    "        # we use below loop to get doc_text which contains all words\n",
    "        # inside headings\n",
    "        for i in headings :\n",
    "            pos = string.find(i)\n",
    "            i = i.strip('\\n').strip('=').replace('/','\\\\').replace(' ','_')\n",
    "            ind.append((i,pos))\n",
    "        ind.append((None,-1))\n",
    "        \n",
    "        page_docs = 0   # num of docs > 180 in length per title\n",
    "                        # just so we get idea how many page_docs\n",
    "                        # qualified at each iteration\n",
    "        for i in range(len(ind)-1) :\n",
    "            \n",
    "            doc_text =  string[ind[i][1]:ind[i+1][1]-1]\n",
    "            # doc_text is content under heading\n",
    "            # if doc_text has more than 180 words ,\n",
    "            # make a document and store in folder\n",
    "            # store file as \n",
    "            # your_Directory/Dog_%breed_name%_%topic_name%\n",
    "            \n",
    "#             print 'name of document '+ ind[i][0]\n",
    "#             print len(doc_text.split())\n",
    "            if len(doc_text.split()) >= 180 :\n",
    "                f = io.open(\"/home/kush/Desktop/Wiki_Data_2/Geography/\" + 'Geography_' + title.replace(' ','_') + ind[i][0], \"w+\", encoding = 'utf-8')\n",
    "                f.write(doc_text)\n",
    "                f.close()\n",
    "                cnt += 1\n",
    "                page_docs +=1\n",
    "        print page_docs\n",
    "        print cnt\n",
    "        ix = ix+1\n",
    "                \n",
    "    except Exception as e:         # IMP\n",
    "        print '!'*100              # also try by uncommenting below\n",
    "        print 'EXCEPTION!!!'       # section     \n",
    "        ix += 1\n",
    "#         print e     \n",
    "        continue\n",
    "#         print 'Enter another string'  # Enter one of the strings from the disambiguation   \n",
    "#         inp = raw_input()             # or 'n' to skip\n",
    "#         if inp == 'n':\n",
    "#             ix = ix + 1\n",
    "#             continue\n",
    "#         else : \n",
    "#             countries[ix] = inp\n",
    "#             continue\n",
    "print 'number of docs = '+ str(cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  List of World Heritage Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDIA\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_World_Heritage_Sites_in_India'\n",
    "r = requests.get(url)\n",
    "soup = BS(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_list = soup.find_all('table',{'class' : 'wikitable'})\n",
    "ind_list = ind_list[0].find_all('tr') # Fetch all rows\n",
    "nrows = len(ind_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "WHS= []\n",
    "for row in ind_list :\n",
    "    name = row.find('a')\n",
    "    if name != None :\n",
    "        WHS.append(name.text)\n",
    "        \n",
    "print len(WHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CANADA\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_World_Heritage_Sites_in_Canada'\n",
    "r = requests.get(url)\n",
    "soup = BS(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_list = soup.find_all('table',{'class' : 'wikitable'})\n",
    "can_list = can_list[0].find_all('tr') # Fetch all rows\n",
    "nrows = len(can_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "for row in can_list :\n",
    "    name = row.find('a')\n",
    "    if name != None :\n",
    "        WHS.append(name.text)\n",
    "        \n",
    "print len(WHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Red Bay Basque Whaling Station', u'Rideau Canal', u'SGang Gwaay', u'Waterton Glacier International Peace Park', u'Wood Buffalo National Park']\n"
     ]
    }
   ],
   "source": [
    "print WHS[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUSSIA\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_World_Heritage_Sites_in_Russia'\n",
    "r = requests.get(url)\n",
    "soup = BS(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_list = soup.find_all('table',{'class' : 'wikitable'})\n",
    "rus_list = can_list[0].find_all('tr') # Fetch all rows\n",
    "nrows = len(rus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "for row in can_list :\n",
    "    name = row.find('a')\n",
    "    if name != None :\n",
    "        WHS.append(name.text)\n",
    "        \n",
    "print len(WHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
